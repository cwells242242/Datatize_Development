{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "100%|██████████| 2/2 [00:00<00:00, 13.98it/s]\n",
      "\n",
      "100%|██████████| 2/2 [00:00<00:00, 124.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ingest] Loaded 17 Documents from C:\\Users\\cwell\\OneDrive\\Desktop\\Datatize\\Test_Files\n",
      "[split] Produced 23 chunks (size=1200, overlap=200)\n",
      "[export] Wrote 23 chunks to C:\\Users\\cwell\\OneDrive\\Desktop\\Datatize\\Test_Jupyters\\chunks_output.jsonl\n",
      "\n",
      "--- SAMPLE CHUNK ---\n",
      "SOURCE: C:\\Users\\cwell\\OneDrive\\Desktop\\Datatize\\Test_Files\\Example_1.pdf\n",
      "PAGE: 0\n",
      "CHUNK_ID: 0\n",
      "Statement of Work (SOW) \n",
      " \n",
      "Project: Data Roadmap & AWS Infrastructure Modernization \n",
      "Client: LogiTech \n",
      "Consultant: ConsultingCo \n",
      "Date: August 27, 2025 \n",
      "Duration: 6 Months \n",
      "Total Cost: $1,000,000 \n",
      " \n",
      "Introduction \n",
      " \n",
      "This Statement of Work (“SOW”) outlines the objectives, scope, deliverables, timeline, ...\n",
      "\n",
      "--- SAMPLE CHUNK ---\n",
      "SOURCE: C:\\Users\\cwell\\OneDrive\\Desktop\\Datatize\\Test_Files\\Example_1.pdf\n",
      "PAGE: 1\n",
      "CHUNK_ID: 1\n",
      "Migrate legacy systems and data workloads to AWS. \n",
      " \n",
      "Provide best practices for governance, compliance, and cost optimization. \n",
      " \n",
      "Enable knowledge transfer and training for LogiTech teams. \n",
      " \n",
      "Scope of Work \n",
      "In-Scope \n",
      " \n",
      "Current-State Assessment: Review existing infrastructure, data systems, and workl ...\n",
      "\n",
      "--- SAMPLE CHUNK ---\n",
      "SOURCE: C:\\Users\\cwell\\OneDrive\\Desktop\\Datatize\\Test_Files\\Example_1.pdf\n",
      "PAGE: 2\n",
      "CHUNK_ID: 2\n",
      "Ongoing operational support post-migration (may be defined in a separate Managed \n",
      "Services agreement). \n",
      " \n",
      "Non-AWS platform migrations. \n",
      " \n",
      "Custom application development outside of agreed-upon refactoring requirements. \n",
      " \n",
      "Deliverables \n",
      " \n",
      "Discovery Report & Assessment Findings (Month 1) \n",
      " \n",
      "Data Modern ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ingest_and_chunk.py\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# LangChain ingestion primitives\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load mixed documents\n",
    "# -----------------------\n",
    "def load_docs_from_directory(root: str) -> List:\n",
    "    \"\"\"\n",
    "    Loads PDFs and DOCX files from a directory into LangChain Document objects,\n",
    "    carrying useful metadata like source path and (for PDFs) page numbers.\n",
    "    \"\"\"\n",
    "    root_path = Path(root).resolve()\n",
    "    if not root_path.exists():\n",
    "        raise FileNotFoundError(f\"Path not found: {root_path}\")\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # PDF: one Document per page, metadata includes {\"page\": int, \"source\": str}\n",
    "    pdf_loader = DirectoryLoader(\n",
    "        str(root_path),\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True,\n",
    "        use_multithreading=True,\n",
    "        max_concurrency=8,\n",
    "    )\n",
    "    docs.extend(pdf_loader.load())\n",
    "\n",
    "    # DOCX: one Document per file, metadata includes {\"source\": str}\n",
    "    docx_loader = DirectoryLoader(\n",
    "        str(root_path),\n",
    "        glob=\"**/*.docx\",\n",
    "        loader_cls=Docx2txtLoader,\n",
    "        show_progress=True,\n",
    "        use_multithreading=True,\n",
    "        max_concurrency=8,\n",
    "    )\n",
    "    docs.extend(docx_loader.load())\n",
    "\n",
    "    print(f\"[ingest] Loaded {len(docs)} Documents from {root_path}\")\n",
    "    return docs\n",
    "\n",
    "# -------------------------\n",
    "# 2) Split into useful bits\n",
    "# -------------------------\n",
    "def split_docs(docs: List, chunk_size=1200, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Splits documents into overlapping chunks while preserving metadata.\n",
    "    Overlap helps avoid context being cut in half at chunk boundaries.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # coarse -> fine\n",
    "        add_start_index=True,  # adds 'start_index' to metadata for traceability in the source text\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # Ensure minimal provenance on every chunk\n",
    "    for i, d in enumerate(chunks):\n",
    "        d.metadata.setdefault(\"chunk_id\", i)\n",
    "        d.metadata.setdefault(\"source\", d.metadata.get(\"source\", \"unknown\"))\n",
    "    print(f\"[split] Produced {len(chunks)} chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    return chunks\n",
    "\n",
    "# -------------------------\n",
    "# 3) Save chunks to JSONL\n",
    "# -------------------------\n",
    "def save_chunks_jsonl(chunks: List, out_path=\"chunks_output.jsonl\"):\n",
    "    \"\"\"\n",
    "    Saves chunks to a JSONL file with page_content + metadata.\n",
    "    This is the most portable form for future pipelines.\n",
    "    \"\"\"\n",
    "    out_file = Path(out_path).resolve()\n",
    "    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with out_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for d in chunks:\n",
    "            record = {\n",
    "                \"text\": d.page_content,\n",
    "                \"metadata\": d.metadata,\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[export] Wrote {len(chunks)} chunks to {out_file}\")\n",
    "    return str(out_file)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Glue it together\n",
    "# -------------------------\n",
    "def main():\n",
    "    SRC_DIR = \"../Test_Files\"           # your folder\n",
    "    OUT_JSONL = \"./chunks_output.jsonl\" # where to save the extracted info\n",
    "\n",
    "    docs = load_docs_from_directory(SRC_DIR)\n",
    "    chunks = split_docs(docs, chunk_size=1200, chunk_overlap=200)\n",
    "    save_chunks_jsonl(chunks, OUT_JSONL)\n",
    "\n",
    "    # Optional: quick peek at a couple of chunks for sanity\n",
    "    for d in chunks[:3]:\n",
    "        print(\"\\n--- SAMPLE CHUNK ---\")\n",
    "        print(\"SOURCE:\", d.metadata.get(\"source\"))\n",
    "        if \"page\" in d.metadata:\n",
    "            print(\"PAGE:\", d.metadata[\"page\"])\n",
    "        print(\"CHUNK_ID:\", d.metadata.get(\"chunk_id\"))\n",
    "        print(d.page_content[:300], \"...\" if len(d.page_content) > 300 else \"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromeadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3eb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
